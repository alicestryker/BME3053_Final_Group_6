{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8190efd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports - DO NOT CHANGE\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd34f76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeukemiaDetectionModel:\n",
    "    def __init__(self, target_column, id_column=None):\n",
    "        \"\"\"Initialize model with flexible column names\"\"\"\n",
    "        self.target_column = target_column\n",
    "        self.id_column = id_column\n",
    "        self.label_encoders = {}\n",
    "        self.scaler = StandardScaler()\n",
    "        self.trained_models = None\n",
    "\n",
    "    def load_data(self, data_path=None, dataframe=None):\n",
    "        \"\"\"Load data from either a CSV file or existing dataframe\"\"\"\n",
    "        if data_path is not None:\n",
    "            if data_path.endswith('.csv'):\n",
    "                self.df = pd.read_csv(data_path)\n",
    "            elif data_path.endswith('.xlsx'):\n",
    "                self.df = pd.read_excel(data_path)\n",
    "            elif data_path.endswith('.json'):\n",
    "                self.df = pd.read_json(data_path)\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported file format. Please provide a CSV, Excel, or JSON file.\")\n",
    "        elif dataframe is not None:\n",
    "            self.df = dataframe.copy()\n",
    "        else:\n",
    "            raise ValueError(\"Either data_path or dataframe must be provided\")\n",
    "        \n",
    "        # Verify target column exists\n",
    "        if self.target_column not in self.df.columns:\n",
    "            raise ValueError(f\"Target column '{self.target_column}' not found in data\")\n",
    "        \n",
    "        # Handle missing values\n",
    "        if self.df.isnull().sum().sum() > 0:\n",
    "            self.df.fillna(self.df.mean(numeric_only=True), inplace=True)\n",
    "            self.df.fillna('Unknown', inplace=True)\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def explore_data(self, max_plots=6):\n",
    "        \"\"\"Explore data with automatic feature type detection\"\"\"\n",
    "        print(\"\\nClass Distribution for\", self.target_column)\n",
    "        print(self.df[self.target_column].value_counts(normalize=True))\n",
    "        \n",
    "        # Convert target to numerical if it's categorical\n",
    "        if self.df[self.target_column].dtype == 'object':\n",
    "            target_encoder = LabelEncoder()\n",
    "            self.df[self.target_column] = target_encoder.fit_transform(self.df[self.target_column])\n",
    "            print(\"\\nConverted target values:\")\n",
    "            for i, label in enumerate(target_encoder.classes_):\n",
    "                print(f\"{label} -> {i}\")\n",
    "        \n",
    "        # Automatically identify numerical and categorical columns\n",
    "        self.numerical_features = self.df.select_dtypes(\n",
    "            include=['int64', 'float64']).columns.tolist()\n",
    "        self.categorical_features = self.df.select_dtypes(\n",
    "            include=['object', 'category']).columns.tolist()\n",
    "        \n",
    "        # Remove target and ID columns from features\n",
    "        for col in [self.target_column, self.id_column]:\n",
    "            if col in self.numerical_features:\n",
    "                self.numerical_features.remove(col)\n",
    "            if col in self.categorical_features:\n",
    "                self.categorical_features.remove(col)\n",
    "        \n",
    "        print(\"\\nNumerical features:\", self.numerical_features)\n",
    "        print(\"Categorical features:\", self.categorical_features)\n",
    "        \n",
    "        # Plot distributions of numerical features\n",
    "        if len(self.numerical_features) > 0:\n",
    "            n_plots = min(len(self.numerical_features), max_plots)\n",
    "            plt.figure(figsize=(15, 10))\n",
    "            for i, feature in enumerate(self.numerical_features[:n_plots], 1):\n",
    "                plt.subplot(2, 3, i)\n",
    "                sns.histplot(data=self.df, x=feature, hue=self.target_column, multiple=\"stack\")\n",
    "                plt.title(f'Distribution of {feature}')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        # Correlation matrix\n",
    "        if len(self.numerical_features) > 0:\n",
    "            numerical_df = self.df[self.numerical_features + [self.target_column]]\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            sns.heatmap(numerical_df.corr(), annot=True, cmap='coolwarm', center=0)\n",
    "            plt.title('Correlation Matrix')\n",
    "            plt.show()\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def preprocess_data(self, test_size=0.2, random_state=42):\n",
    "        \"\"\"Preprocess data with automatic handling of different data types\"\"\"\n",
    "        df_processed = self.df.copy()\n",
    "        \n",
    "        # Encode categorical variables\n",
    "        for feature in self.categorical_features:\n",
    "            self.label_encoders[feature] = LabelEncoder()\n",
    "            df_processed[feature] = self.label_encoders[feature].fit_transform(df_processed[feature])\n",
    "        \n",
    "        # Prepare features and target\n",
    "        exclude_cols = [col for col in [self.target_column, self.id_column] if col is not None]\n",
    "        X = df_processed.drop(exclude_cols, axis=1)\n",
    "        y = df_processed[self.target_column]\n",
    "        \n",
    "        # Split the data\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=random_state\n",
    "        )\n",
    "        \n",
    "        # Scale the features\n",
    "        self.X_train_scaled = self.scaler.fit_transform(self.X_train)\n",
    "        self.X_test_scaled = self.scaler.transform(self.X_test)\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def train_models(self):\n",
    "        \"\"\"Train multiple models with progress updates\"\"\"\n",
    "        self.models = {\n",
    "            'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "            'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        }\n",
    "        \n",
    "        self.trained_models = {}\n",
    "        for name, model in self.models.items():\n",
    "            print(f\"\\nTraining {name}...\")\n",
    "            model.fit(self.X_train_scaled, self.y_train)\n",
    "            train_score = model.score(self.X_train_scaled, self.y_train)\n",
    "            print(f\"{name} training accuracy: {train_score:.4f}\")\n",
    "            self.trained_models[name] = model\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def evaluate_models(self):\n",
    "        \"\"\"Evaluate models with detailed metrics\"\"\"\n",
    "        self.results = {}\n",
    "        \n",
    "        for name, model in self.trained_models.items():\n",
    "            print(f\"\\nEvaluating {name}...\")\n",
    "            y_pred = model.predict(self.X_test_scaled)\n",
    "            y_pred_proba = model.predict_proba(self.X_test_scaled)[:, 1]\n",
    "            \n",
    "            accuracy = accuracy_score(self.y_test, y_pred)\n",
    "            fpr, tpr, _ = roc_curve(self.y_test, y_pred_proba)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            \n",
    "            self.results[name] = {\n",
    "                'accuracy': accuracy,\n",
    "                'fpr': fpr,\n",
    "                'tpr': tpr,\n",
    "                'auc': roc_auc,\n",
    "                'predictions': y_pred,\n",
    "                'probabilities': y_pred_proba\n",
    "            }\n",
    "\n",
    "        # Plot ROC curves\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        for name, metrics in self.results.items():\n",
    "            plt.plot(metrics['fpr'], metrics['tpr'], \n",
    "                    label=f'{name} (AUC = {metrics[\"auc\"]:.2f})')\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('ROC Curves for All Models')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # Print detailed classification reports\n",
    "        for name, metrics in self.results.items():\n",
    "            print(f\"\\nClassification Report for {name}:\")\n",
    "            print(classification_report(self.y_test, metrics['predictions']))\n",
    "\n",
    "        # Compare model accuracies\n",
    "        accuracies = {name: metrics['accuracy'] \n",
    "                     for name, metrics in self.results.items()}\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(accuracies.keys(), accuracies.values())\n",
    "        plt.title('Model Accuracy Comparison')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def _load_data_from_path(self, data_path):\n",
    "        \"\"\"Helper method to load data from a file path.\"\"\"\n",
    "        if data_path.endswith('.csv'):\n",
    "            return pd.read_csv(data_path)\n",
    "        elif data_path.endswith('.xlsx'):\n",
    "            return pd.read_excel(data_path)\n",
    "        elif data_path.endswith('.json'):\n",
    "            return pd.read_json(data_path)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file format. Please provide a CSV, Excel, or JSON file.\")\n",
    "\n",
    "    def predict_unlabeled_data(self, data_path=None, dataframe=None, save_to=None):\n",
    "        \"\"\"Predict leukemia status for unlabeled data.\"\"\"\n",
    "        # Load data from file or dataframe\n",
    "        if data_path is not None:\n",
    "            df_processed = self._load_data_from_path(data_path)\n",
    "        elif dataframe is not None:\n",
    "            df_processed = dataframe.copy()\n",
    "        else:\n",
    "            raise ValueError(\"Either data_path or dataframe must be provided\")\n",
    "        \n",
    "        # Encode categorical variables\n",
    "        for feature in self.categorical_features:\n",
    "            if feature in df_processed.columns:\n",
    "                if feature in self.label_encoders:\n",
    "                    df_processed[feature] = self.label_encoders[feature].transform(df_processed[feature])\n",
    "                else:\n",
    "                    raise ValueError(f\"Feature '{feature}' was not seen during training\")\n",
    "        \n",
    "        # Scale numerical features\n",
    "        X_unlabeled = df_processed.drop(columns=[self.target_column, self.id_column], errors='ignore')\n",
    "        X_unlabeled_scaled = self.scaler.transform(X_unlabeled)\n",
    "        \n",
    "        # Use the trained model to predict\n",
    "        predictions = {}\n",
    "        for name, model in self.trained_models.items():\n",
    "            predictions[name] = model.predict(X_unlabeled_scaled)\n",
    "        \n",
    "        # Save predictions to a file if specified\n",
    "        if save_to:\n",
    "            pd.DataFrame(predictions).to_csv(save_to, index=False)\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c2ed1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and run the model\n",
    "model = LeukemiaDetectionModel(\n",
    "    target_column='Leukemia_Status',\n",
    "    id_column='Patient_ID'\n",
    ")\n",
    "\n",
    "model.load_data(\n",
    "    data_path='biased_leukemia_dataset.csv'\n",
    ")\n",
    "\n",
    "model.explore_data()\n",
    "\n",
    "model.preprocess_data(test_size=0.2)\n",
    "\n",
    "model.train_models()\n",
    "\n",
    "model.evaluate_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77caec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict leukemia status for a new dataset\n",
    "new_data_path = 'unlabeled_leukemia_dataset.csv'  # Path to the new dataset\n",
    "predictions = model.predict_unlabeled_data(data_path=new_data_path)\n",
    "\n",
    "# Display predictions for each model\n",
    "for model_name, preds in predictions.items():\n",
    "    print(f\"Predictions from {model_name}:\")\n",
    "    print(preds[:10])  # Display the first 10 predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
