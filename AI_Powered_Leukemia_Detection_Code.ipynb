{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d97c2a9d",
   "metadata": {},
   "source": [
    "# ü©∫ AI Powered Leukemia Detection Model\n",
    "This notebook implements an AI-powered model to detect leukemia using machine learning techniques. It includes data preprocessing, model training, evaluation, and prediction functionalities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0f1e00",
   "metadata": {},
   "source": [
    "# üõ†Ô∏è Standard Imports\n",
    "This cell imports all the necessary libraries and modules required for data processing, visualization, and machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8190efd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports - DO NOT CHANGE\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ed5d61",
   "metadata": {},
   "source": [
    "# üß† Leukemia Detection Model Class\n",
    "This cell defines the `LeukemiaDetectionModel` class, which includes methods for loading data, preprocessing, training models, and making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd34f76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LeukemiaDetectionModel class - DO NOT CHANGE\n",
    "class LeukemiaDetectionModel:\n",
    "    def __init__(self, target_column=None, id_column=None):\n",
    "        \"\"\"Initialize model with optional column detection\"\"\"\n",
    "        self.target_column = target_column\n",
    "        self.id_column = id_column\n",
    "        self.label_encoders = {}\n",
    "        self.scaler = StandardScaler()\n",
    "        self.trained_models = None\n",
    "\n",
    "    def auto_detect_columns(self):\n",
    "        \"\"\"Automatically detect target and ID columns based on keywords\"\"\"\n",
    "        if self.target_column is None:\n",
    "            for col in self.df.columns:\n",
    "                if 'leukemia' in col.lower() and 'status' in col.lower():\n",
    "                    self.target_column = col\n",
    "                    print(f\"Detected target column: {self.target_column}\")\n",
    "                    break\n",
    "            if self.target_column is None:\n",
    "                raise ValueError(\"Unable to detect target column related to leukemia status. Please specify it explicitly.\")\n",
    "\n",
    "        if self.id_column is None:\n",
    "            for col in self.df.columns:\n",
    "                if 'id' in col.lower():\n",
    "                    self.id_column = col\n",
    "                    print(f\"Detected ID column: {self.id_column}\")\n",
    "                    break\n",
    "\n",
    "    def load_data(self, data_path=None):\n",
    "        \"\"\"Load data and detect columns if necessary\"\"\"\n",
    "        if data_path is not None:\n",
    "            if data_path.endswith('.csv'):\n",
    "                self.df = pd.read_csv(data_path)\n",
    "            elif data_path.endswith('.xlsx'):\n",
    "                self.df = pd.read_excel(data_path)\n",
    "            elif data_path.endswith('.json'):\n",
    "                self.df = pd.read_json(data_path)\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported file format. Please provide a CSV, Excel, or JSON file.\")\n",
    "        else:\n",
    "            raise ValueError(\"data_path must be provided\")\n",
    "\n",
    "        # Automatically detect target and ID columns if not provided\n",
    "        self.auto_detect_columns()\n",
    "\n",
    "        # Handle missing values\n",
    "        if self.df.isnull().sum().sum() > 0:\n",
    "            self.df.fillna(self.df.mean(numeric_only=True), inplace=True)\n",
    "            self.df.fillna('Unknown', inplace=True)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def explore_data(self, max_plots=6):\n",
    "        \"\"\"Explore data with automatic feature type detection\"\"\"\n",
    "        print(\"\\nClass Distribution for\", self.target_column)\n",
    "        print(self.df[self.target_column].value_counts(normalize=True))\n",
    "        \n",
    "        # Convert target to numerical if it's categorical\n",
    "        if self.df[self.target_column].dtype == 'object':\n",
    "            target_encoder = LabelEncoder()\n",
    "            self.df[self.target_column] = target_encoder.fit_transform(self.df[self.target_column])\n",
    "            print(\"\\nConverted target values:\")\n",
    "            for i, label in enumerate(target_encoder.classes_):\n",
    "                print(f\"{label} -> {i}\")\n",
    "        \n",
    "        # Automatically identify numerical and categorical columns\n",
    "        self.numerical_features = self.df.select_dtypes(\n",
    "            include=['int64', 'float64']).columns.tolist()\n",
    "        self.categorical_features = self.df.select_dtypes(\n",
    "            include=['object', 'category']).columns.tolist()\n",
    "        \n",
    "        # Remove target and ID columns from features\n",
    "        for col in [self.target_column, self.id_column]:\n",
    "            if col in self.numerical_features:\n",
    "                self.numerical_features.remove(col)\n",
    "            if col in self.categorical_features:\n",
    "                self.categorical_features.remove(col)\n",
    "        \n",
    "        print(\"\\nNumerical features:\", self.numerical_features)\n",
    "        print(\"Categorical features:\", self.categorical_features)\n",
    "        \n",
    "        # Plot distributions of numerical features\n",
    "        if len(self.numerical_features) > 0:\n",
    "            n_plots = min(len(self.numerical_features), max_plots)\n",
    "            plt.figure(figsize=(15, 10))\n",
    "            for i, feature in enumerate(self.numerical_features[:n_plots], 1):\n",
    "                plt.subplot(2, 3, i)\n",
    "                sns.histplot(data=self.df, x=feature, hue=self.target_column, multiple=\"stack\")\n",
    "                plt.title(f'Distribution of {feature}')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        df_processed = self.df.copy()\n",
    "\n",
    "        # Encode categorical variables\n",
    "        for feature in self.categorical_features:\n",
    "            self.label_encoders[feature] = LabelEncoder()\n",
    "            df_processed[feature] = self.label_encoders[feature].fit_transform(df_processed[feature])\n",
    "\n",
    "        # Prepare features and target\n",
    "        exclude_cols = [col for col in [self.target_column, self.id_column] if col is not None]\n",
    "        X = df_processed.drop(exclude_cols, axis=1).values\n",
    "        y = df_processed[self.target_column].values\n",
    "\n",
    "        # Use the entire dataset for training\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "\n",
    "        # Scale the features\n",
    "        self.X_train_scaled = self.scaler.fit_transform(self.X_train)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def select_features(self, n_features=10):\n",
    "        \"\"\"Use all features without selection.\"\"\"\n",
    "        if not hasattr(self, 'X_train_scaled') or not hasattr(self, 'y_train'):\n",
    "            raise ValueError(\"Data must be preprocessed before feature selection.\")\n",
    "\n",
    "        # Use all features\n",
    "        feature_names = [col for col in self.df.columns if col not in [self.target_column, self.id_column]]\n",
    "        self.selected_features = feature_names\n",
    "        print(f\"Using all features: {self.selected_features}\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def train_models(self):\n",
    "        \"\"\"Train multiple models with hyperparameter tuning and class weighting\"\"\"\n",
    "        from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "        # Define hyperparameter grids\n",
    "        param_grids = {\n",
    "            'Logistic Regression': {\n",
    "                'C': [0.1, 1, 10],\n",
    "                'penalty': ['l2'],\n",
    "                'solver': ['lbfgs']\n",
    "            },\n",
    "            'Random Forest': {\n",
    "                'n_estimators': [50, 100, 200],\n",
    "                'max_depth': [10, 20, None],\n",
    "                'class_weight': ['balanced', 'balanced_subsample']\n",
    "            }\n",
    "        }\n",
    "\n",
    "        self.models = {\n",
    "            'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "            'Random Forest': RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "        }\n",
    "\n",
    "        self.trained_models = {}\n",
    "        for name, model in self.models.items():\n",
    "            print(f\"\\nTraining {name} with hyperparameter tuning...\")\n",
    "            grid_search = GridSearchCV(model, param_grids[name], cv=3, scoring='precision')\n",
    "            grid_search.fit(self.X_train_scaled, self.y_train)\n",
    "            best_model = grid_search.best_estimator_\n",
    "            train_score = best_model.score(self.X_train_scaled, self.y_train)\n",
    "            print(f\"{name} best parameters: {grid_search.best_params_}\")\n",
    "            print(f\"{name} training precision: {train_score:.4f}\")\n",
    "            self.trained_models[name] = best_model\n",
    "\n",
    "        return self\n",
    "\n",
    "    def evaluate_models(self):\n",
    "        \"\"\"Evaluate models with detailed metrics on the training data.\"\"\"\n",
    "        self.results = {}\n",
    "        for name, model in self.trained_models.items():\n",
    "            print(f\"\\nEvaluating {name}...\")\n",
    "            y_pred_proba = model.predict_proba(self.X_train_scaled)[:, 1]\n",
    "            threshold = 0.6  # Adjusted threshold for higher precision\n",
    "            y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "            accuracy = accuracy_score(self.y_train, y_pred)\n",
    "            self.results[name] = {\n",
    "                'accuracy': accuracy,\n",
    "                'predictions': y_pred\n",
    "            }\n",
    "            print(f\"\\nClassification Report for {name}:\")\n",
    "            print(classification_report(self.y_train, y_pred))\n",
    "        return self\n",
    "\n",
    "    def _load_data_from_path(self, data_path):\n",
    "        \"\"\"Helper method to load data from a file path.\"\"\"\n",
    "        if data_path.endswith('.csv'):\n",
    "            return pd.read_csv(data_path)\n",
    "        elif data_path.endswith('.xlsx'):\n",
    "            return pd.read_excel(data_path)\n",
    "        elif data_path.endswith('.json'):\n",
    "            return pd.read_json(data_path)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file format. Please provide a CSV, Excel, or JSON file.\")\n",
    "\n",
    "    def predict_unlabeled_data(self, user_input):\n",
    "        \"\"\"Predict leukemia status for user-provided input.\"\"\"\n",
    "        # Align user input with training features\n",
    "        aligned_input = [user_input[feature] for feature in self.selected_features]\n",
    "\n",
    "        # Scale numerical features\n",
    "        aligned_input_scaled = self.scaler.transform([aligned_input])\n",
    "\n",
    "        # Use the trained model to predict\n",
    "        predictions = {name: model.predict(aligned_input_scaled)[0] for name, model in self.trained_models.items()}\n",
    "\n",
    "        return predictions\n",
    "    \n",
    "    def select_features(self, n_features=10):\n",
    "        \"\"\"Use all features without selection.\"\"\"\n",
    "        if not hasattr(self, 'X_train_scaled') or not hasattr(self, 'y_train'):\n",
    "            raise ValueError(\"Data must be preprocessed before feature selection.\")\n",
    "\n",
    "        # Use all features\n",
    "        feature_names = [col for col in self.df.columns if col not in [self.target_column, self.id_column]]\n",
    "        self.selected_features = feature_names\n",
    "        print(f\"Using all features: {self.selected_features}\")\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffbd9cc",
   "metadata": {},
   "source": [
    "# üöÄ Run the Leukemia Detection Model\n",
    "This cell creates an instance of the `LeukemiaDetectionModel` class, loads the dataset, explores the data, preprocesses it, selects the top features, trains the models, and evaluates their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e081bb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and run the model with automatic column detection and feature selection\n",
    "model = LeukemiaDetectionModel()\n",
    "\n",
    "model.load_data(\n",
    "    data_path='synthetic_leukemia_biomarkers.csv'  # Replace with your data file path\n",
    ")\n",
    "\n",
    "model.explore_data()\n",
    "\n",
    "model.preprocess_data()\n",
    "\n",
    "# Select the top 10 features\n",
    "model.select_features(n_features=10)\n",
    "\n",
    "model.train_models()\n",
    "\n",
    "model.evaluate_models()\n",
    "\n",
    "# Generate confusion matrix for the model using corrected implementation\n",
    "for name, trained_model in model.trained_models.items():\n",
    "    print(f'Confusion Matrix for {name}:')\n",
    "    y_pred = trained_model.predict(model.X_train_scaled)  # Use training data\n",
    "\n",
    "    # Combine ground truth and predictions into a DataFrame\n",
    "    results_df = pd.DataFrame({\n",
    "        'Ground_Truth': model.y_train,  # Use training labels\n",
    "        'Prediction': y_pred\n",
    "    })\n",
    "\n",
    "    # Display the DataFrame\n",
    "    print(\"Combined Results DataFrame:\")\n",
    "    print(results_df)\n",
    "\n",
    "    # Create and display confusion matrix\n",
    "    cm = confusion_matrix(model.y_train, y_pred)  # Use training labels\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Visualize the confusion matrix\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Has Leukemia', 'Does Not Have Leukemia'])\n",
    "    disp.plot(cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix for {name}')\n",
    "    plt.show()\n",
    "\n",
    "# Instructions for interpreting classification metrics\n",
    "print(\"\\nHow to Interpret the Classification Report:\")\n",
    "print(\"Precision: The proportion of true positive predictions out of all positive predictions. High precision means fewer false positives.\")\n",
    "print(\"Recall: The proportion of true positive predictions out of all actual positives. High recall means fewer false negatives.\")\n",
    "print(\"F1-Score: The harmonic mean of precision and recall. A balanced metric when precision and recall are equally important.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31302a5d",
   "metadata": {},
   "source": [
    "# üîç Predict Leukemia Status\n",
    "This cell allows the user to input data for only the selected features and predicts whether the person has leukemia or not using the trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4710041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict leukemia status for a new dataset using user input with all features\n",
    "def get_user_input():\n",
    "    \"\"\"Prompt user for input for all features and return a dictionary.\"\"\"\n",
    "    print(\"\\nPlease provide the following inputs for the features:\")\n",
    "    user_data = {}\n",
    "    for feature in model.selected_features:\n",
    "        min_val = model.df[feature].min()\n",
    "        max_val = model.df[feature].max()\n",
    "        value = float(input(f\"Enter value for {feature} (Range [{min_val}, {max_val}]): \"))\n",
    "        user_data[feature] = value\n",
    "    return user_data\n",
    "\n",
    "# Get user input and make predictions\n",
    "user_input = get_user_input()\n",
    "\n",
    "# Align user input with all features\n",
    "aligned_input = [user_input[feature] for feature in model.selected_features]\n",
    "\n",
    "# Scale the input and make predictions\n",
    "aligned_input_scaled = model.scaler.transform([aligned_input])\n",
    "predictions = {name: trained_model.predict(aligned_input_scaled)[0] for name, trained_model in model.trained_models.items()}\n",
    "\n",
    "# Convert predictions to human-readable labels and display them\n",
    "print(\"\\nPredictions:\")  # Single print statement for all predictions\n",
    "for model_name, prediction in predictions.items():\n",
    "    result = \"Has Leukemia\" if prediction == 1 else \"Does Not Have Leukemia\"\n",
    "    print(f\"Prediction from {model_name}: {result}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
